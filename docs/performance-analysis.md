# Performance Analysis (mrkit-go vs Hadoop Streaming)

This page summarizes host-vs-host measurements collected on the same machine, for `COUNT` workload (`100,000` rows, `10,000` keys, `3` rounds, median used).
The goal is to show performance behavior under comparable local conditions.

## Scope

- Compared tools:
  - `mrkit` (legacy runner path)
  - `mrkit_bin` (optimized binary-path run)
  - `hadoop_streaming_host` (local Hadoop Streaming)
- Data source files (repo-relative benchmark artifacts):
  - `benchmark/hadoop-host/results_*/count_compare.csv`
  - `benchmark/hadoop-host/results_bin_*/count_compare.csv`
- Metrics:
  - `wall_ms`: end-to-end elapsed time
  - `input_rps`: processed rows / second

## Runtime Conditions

To interpret numbers correctly, keep runtime context explicit:

- Execution mode:
  - host-vs-host (both mrkit and Hadoop run on the same host OS, not in Docker)
  - single-machine local mode (`mapreduce.framework.name=local` for Hadoop Streaming)
- Workload:
  - `COUNT` task only
  - `100,000` input rows, `10,000` logical keys
  - 3 rounds per reducer setting, median reported
- Dataset and scripts:
  - generated by repository scripts under `scripts/`
  - outputs written under `benchmark/hadoop-host/...`
- System and tooling:
  - macOS host (Darwin) in this validation session
  - Go runtime managed by `g` on host
  - local Hadoop installation on host
- Network/storage context:
  - local filesystem input/output
  - no distributed HDFS/YARN cluster in this comparison

Because the benchmark is local-mode and single-host, results represent engineering baseline behavior, not cluster-scale throughput.

## Median Matrix

### A) Runner path (`mrkit`) vs Hadoop Streaming

| Reducers | mrkit wall_ms | mrkit rps | hadoop wall_ms | hadoop rps |
|---|---:|---:|---:|---:|
| 1 | 8523 | 11,732.96 | 2093 | 47,778.31 |
| 4 | 2637 | 37,921.88 | 1911 | 52,328.62 |
| 8 | 2598 | 38,491.15 | 1889 | 52,938.06 |

Observation:
- In legacy runner mode, Hadoop Streaming is faster in this benchmark setup.

### B) Optimized binary path (`mrkit_bin`) vs Hadoop Streaming

| Reducers | mrkit_bin wall_ms | mrkit_bin rps | hadoop wall_ms | hadoop rps |
|---|---:|---:|---:|---:|
| 1 | 550 | 181,818.18 | 1909 | 52,383.45 |
| 4 | 562 | 177,935.94 | 1892 | 52,854.12 |
| 8 | 1576 | 63,451.78 | 1863 | 53,676.87 |

Observation:
- With optimized binary-path execution, `mrkit_bin` outperforms Hadoop Streaming in reducers `1/4`, and remains competitive at `8`.
- Reducers `8` is sensitive to local scheduling and runtime overhead on this host.

## Key Conclusion (Same Host, Same Data)

Using the optimized `mrkit_bin` path, mrkit-go shows clear throughput advantage in this benchmark:

- reducers=1: `181,818` rps vs Hadoop `52,383` rps (about `3.47x`)
- reducers=4: `177,935` rps vs Hadoop `52,854` rps (about `3.37x`)
- reducers=8: `63,451` rps vs Hadoop `53,676` rps (about `1.18x`)

In this workload and environment, the core implementation is faster than local Hadoop Streaming at low/medium reducer counts.

## Why Modes Differ

- Legacy runner path includes framework overhead (startup, RPC scheduling, plugin/runtime orchestration).
- Hadoop Streaming has mature local sort/shuffle pipeline even in local mode.
- Reducer count increase does not always improve throughput; local CPU, disk, and context switching become bottlenecks.

## Why Direct `go run` Can Look Slow

Direct `go run` in tight benchmark loops includes extra overhead that is not part of steady-state data processing:

- per-run compile/link overhead for command binaries
- plugin/builtin preparation overhead
- process startup and initialization overhead

Measured impact in this repo (same data, same host), comparing `mrkit` runner path vs `mrkit_bin`:

- reducers=1: `11,732` rps vs `181,818` rps (`~15.5x` gap)
- reducers=4: `37,921` rps vs `177,935` rps (`~4.7x` gap)
- reducers=8: `38,491` rps vs `63,451` rps (`~1.65x` gap)

So `go run` is suitable for development checks, but can significantly underestimate production throughput.

## Practical Recommendations

For fair benchmarking and production-like runs:

1. Build once, then run binary:

```bash
go build -o ./bin/batch ./cmd/batch
./bin/batch -check -config /path/to/flow.json
./bin/batch -config /path/to/flow.json
```

2. Use stable benchmark settings:
- fixed dataset size and key cardinality
- warm-up run before formal measurement
- at least 3 rounds, use median

3. Tune reducers/workers by workload:
- do not assume higher parallelism is always better
- monitor CPU, memory, and I/O saturation together with throughput

4. Use `go run` for functional checks and prebuilt binaries for performance baselines.

## Repro Command Reference

Use your local scripts (already used in this repo workflow):

```bash
# runner-path matrix
ROWS=100000 REDUCERS=1 ROUNDS=3 ./scripts/run_host_hadoop_vs_mrkit_count.sh
ROWS=100000 REDUCERS=4 ROUNDS=3 ./scripts/run_host_hadoop_vs_mrkit_count.sh
ROWS=100000 REDUCERS=8 ROUNDS=3 ./scripts/run_host_hadoop_vs_mrkit_count.sh

# binary-path matrix
ROWS=100000 REDUCERS=1 ROUNDS=3 ./scripts/run_host_hadoop_vs_mrkit_count_bin.sh
ROWS=100000 REDUCERS=4 ROUNDS=3 ./scripts/run_host_hadoop_vs_mrkit_count_bin.sh
ROWS=100000 REDUCERS=8 ROUNDS=3 ./scripts/run_host_hadoop_vs_mrkit_count_bin.sh
```

Optional environment overrides:

```bash
GO_BIN=/path/to/go \
HADOOP_STREAMING_JAR=/path/to/hadoop-streaming-*.jar \
ROWS=100000 REDUCERS=4 ROUNDS=3 \
./scripts/run_host_hadoop_vs_mrkit_count.sh
```

Output files are written under:

- `benchmark/hadoop-host/results_*/count_compare.csv`
- `benchmark/hadoop-host/results_bin_*/count_compare.csv`

If you want stronger confidence, repeat with larger scales and add CPU/memory sampling.
